# Libros
@book{maclane2012homology,
	title={Homology},
	author={MacLane, Saunders},
	year={2012},
	publisher={Springer Science \& Business Media}
}

@book{lee2010introduction,
	title={Introduction to topological manifolds},
	author={Lee, John},
	volume={202},
	year={2010},
	publisher={Springer Science \& Business Media}
}

@book{mac2013categories,
	title={Categories for the working mathematician},
	author={Mac Lane, Saunders},
	volume={5},
	year={2013},
	publisher={Springer Science \& Business Media}
}

@book{dummit2004abstract,
	title={Abstract algebra},
	author={Dummit, David Steven and Foote, Richard M},
	volume={3},
	year={2004},
	publisher={Wiley Hoboken}
}

@article{eilenberg1945general,
	title={General theory of natural equivalences},
	author={Eilenberg, Samuel and MacLane, Saunders},
	journal={Transactions of the American Mathematical Society},
	volume={58},
	pages={231--294},
	year={1945}
}
@book{munkres2018elements,
	title={Elements of algebraic topology},
	author={Munkres, James R},
	year={2018},
	publisher={CRC press}
}

@Inbook{Armstrong1983,
	author="Armstrong, M. A.",
	title="Simplicial Homology",
	bookTitle="Basic Topology",
	year="1983",
	publisher="Springer New York",
	address="New York, NY",
	pages="173--193",
	abstract="If we wish to distinguish between the sphere and the torus, we have already seen one way of doing so using the fundamental group. Any loop in the sphere can be continuously shrunk to a point, in other words the sphere is simply connected, whereas this is not the case for the torus. The fundamental group is a very valuable tool, but it has a significant defect. Remember that the fundamental group of a polyhedron depends only on the 2-skeleton of the underlying complex, making it ideal for studying questions which are essentially two-dimensional (say distinguishing between two surfaces), but leaving it impotent in the face of a problem such as showing that S3 and S4 are not homeomorphic.",
	isbn="978-1-4757-1793-8",
	doi="10.1007/978-1-4757-1793-8_8",
	url="https://doi.org/10.1007/978-1-4757-1793-8_8"
}

@book{rafael2003elementos,
  title={Elementos de la teor{\'\i}a de homolog{\'\i}a cl{\'a}sica},
  author={Rafael Ayala, A.Q.T.A.Q.E.D.},
  isbn={9788447207053},
  series={Serie Ciencias / Universidad de Sevilla},
  url={https://books.google.es/books?id=CAOjRFAMJFUC},
  year={2003},
  publisher={Secretariado de Publicaciones, Universidad de Sevilla}
}

@article {MR0030759,
	AUTHOR = {Whitehead, J. H. C.},
	TITLE = {Combinatorial homotopy. {I}},
	JOURNAL = {Bull. Amer. Math. Soc.},
	FJOURNAL = {Bulletin of the American Mathematical Society},
	VOLUME = {55},
	YEAR = {1949},
	PAGES = {213--245},
	ISSN = {0002-9904},
	MRCLASS = {56.0X},
	MRNUMBER = {30759},
	MRREVIEWER = {H.\ Samelson},
	DOI = {10.1090/S0002-9904-1949-09175-9},
	URL = {https://doi.org/10.1090/S0002-9904-1949-09175-9},
}

@article{webb1985decomposition,
	title={Decomposition of graded modules},
	author={Webb, Cary},
	journal={Proceedings of the American Mathematical Society},
	volume={94},
	number={4},
	pages={565--571},
	year={1985}
}

@article{cauchy1847methode,
	title={M{\'e}thode g{\'e}n{\'e}rale pour la r{\'e}solution des systemes d’{\'e}quations simultan{\'e}es}, author={Cauchy, Augustin},
	journal={Comp. Rend. Sci. Paris},
	volume={25},
	number={1847},
	pages={536--538},
	year={1847}
}

@article{rumelhart1986learning,
	title={Learning representations by back-propagating errors},
	author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
	journal={nature},
	volume={323},
	number={6088},
	pages={533--536},
	year={1986},
	publisher={Nature Publishing Group UK London}
}

@article{duchi2011adaptive,
	title={Adaptive subgradient methods for online learning and stochastic optimization.},
	author={Duchi, John and Hazan, Elad and Singer, Yoram},
	journal={Journal of machine learning research},
	volume={12},
	number={7},
	year={2011}
}

@article{hinton2012lecture,
	title={Lecture 6a overview of mini--batch gradient descent},
	author={Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
	journal={Coursera Lecture slides https://class. coursera. org/neuralnets-2012-001/lecture,[Online},
	year={2012}
}

@article{kingma2014adam,
	title={Adam: A method for stochastic optimization},
	author={Kingma, Diederik P and Ba, Jimmy},
	journal={arXiv preprint arXiv:1412.6980},
	year={2014}
}

@article{Dumoulin2016AGT,
	title={A guide to convolution arithmetic for deep learning},
	author={Vincent Dumoulin and Francesco Visin},
	journal={ArXiv},
	year={2016},
	volume={abs/1603.07285},
	url={https://api.semanticscholar.org/CorpusID:6662846}
}

@book{russell2016artificial,
	abstract = {Artificial Intelligence (AI) is a big field, and this is a big book. We have tried to explore the full breadth of the field, which encompasses logic, probability, and continuous mathematics; perception, reasoning, learning, and action; and everything from microelectronic devices to robotic planetary explorers. The book is also big because we go into some depth. The subtitle of this book is “A Modern Approach.” The intended meaning of this rather empty phrase is that we have tried to synthesize what is now known into a common framework, rather than trying to explain each subfield of AI in its own historical context. We apologize to those whose subfields are, as a result, less recognizable.},
	added-at = {2019-09-20T13:17:25.000+0200},
	address = {England},
	author = {Russell, Stuart Jonathan and Norvig, Peter},
	biburl = {https://www.bibsonomy.org/bibtex/247346452f6de23d8adf304bc3c01eccd/jpmor},
	edition = {Third},
	interhash = {689e5ae1cbb8870d0c5e9e675a651591},
	intrahash = {47346452f6de23d8adf304bc3c01eccd},
	isbn = {9781292153964},
	keywords = {artificial-intelligence etextbook real textbook},
	language = {English},
	publisher = {Pearson Education},
	timestamp = {2020-10-07T13:36:50.000+0200},
	title = {Artificial Intelligence: A Modern Approach},
	url = {https://www.pearson.com/us/higher-education/program/PGM156683.html},
	year = 2016
}

@misc{ramachandran2017searching,
	title={Searching for Activation Functions}, 
	author={Prajit Ramachandran and Barret Zoph and Quoc V. Le},
	year={2017},
	eprint={1710.05941},
	archivePrefix={arXiv},
	primaryClass={cs.NE}
}

@article{bottou2018optimization,
	title={Optimization methods for large-scale machine learning},
	author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
	journal={SIAM review},
	volume={60},
	number={2},
	pages={223--311},
	year={2018},
	publisher={SIAM}
}

@article{hubel1962receptive,
	title={Receptive fields, binocular interaction and functional architecture in the cat's visual cortex},
	author={Hubel, David H and Wiesel, Torsten N},
	journal={The Journal of physiology},
	volume={160},
	number={1},
	pages={106},
	year={1962},
	publisher={Wiley-Blackwell}
}

@article{fukushima1980neocognitron,
	title={Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
	author={Fukushima, Kunihiko},
	journal={Biological cybernetics},
	volume={36},
	number={4},
	pages={193--202},
	year={1980},
	publisher={Springer}
}

@article{serre2007feedforward,
	title={A feedforward architecture accounts for rapid categorization},
	author={Serre, Thomas and Oliva, Aude and Poggio, Tomaso},
	journal={Proceedings of the national academy of sciences},
	volume={104},
	number={15},
	pages={6424--6429},
	year={2007},
	publisher={National Acad Sciences}
}

@ARTICLE{lecun1998gradient,
	author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	journal={Proceedings of the IEEE}, 
	title={Gradient-based learning applied to document recognition}, 
	year={1998},
	volume={86},
	number={11},
	pages={2278-2324},
	keywords={Neural networks;Pattern recognition;Machine learning;Optical character recognition software;Character recognition;Feature extraction;Multi-layer neural network;Optical computing;Hidden Markov models;Principal component analysis},
	doi={10.1109/5.726791}
}

@ARTICLE{lecun1989backpropagation,
	author={LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
	journal={Neural Computation}, 
	title={Backpropagation Applied to Handwritten Zip Code Recognition}, 
	year={1989},
	volume={1},
	number={4},
	pages={541-551},
	keywords={},
	doi={10.1162/neco.1989.1.4.541}
}

@article{krizhevsky2012imagenet,
	title={Imagenet classification with deep convolutional neural networks},
	author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	journal={Advances in neural information processing systems},
	volume={25},
	year={2012}
}

@article{simonyan2014very,
	title={Very deep convolutional networks for large-scale image recognition},
	author={Simonyan, Karen and Zisserman, Andrew},
	journal={arXiv preprint arXiv:1409.1556},
	year={2014}
}

@inproceedings{szegedy2015going,
	title={Going deeper with convolutions},
	author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={1--9},
	year={2015}
}

@inproceedings{he2016deep,
	title={Deep residual learning for image recognition},
	author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={770--778},
	year={2016}
}

@inproceedings{huang2017densely,
	title={Densely connected convolutional networks},
	author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={4700--4708},
	year={2017}
}

@inproceedings{tan2019efficientnet,
	title={Efficientnet: Rethinking model scaling for convolutional neural networks},
	author={Tan, Mingxing and Le, Quoc},
	booktitle={International conference on machine learning},
	pages={6105--6114},
	year={2019},
	organization={PMLR}
}
@article{TANG2024105605,
	title = {HTC-Net: A hybrid CNN-transformer framework for medical image segmentation},
	journal = {Biomedical Signal Processing and Control},
	volume = {88},
	pages = {105605},
	year = {2024},
	issn = {1746-8094},
	doi = {https://doi.org/10.1016/j.bspc.2023.105605},
	url = {https://www.sciencedirect.com/science/article/pii/S1746809423010388},
	author = {Hui Tang and Yuanbin Chen and Tao Wang and Yuanbo Zhou and Longxuan Zhao and Qinquan Gao and Min Du and Tao Tan and Xinlin Zhang and Tong Tong},
	keywords = {Medical image segmentation, Deep convolutional neural networks, Contextual information, Attention},
	abstract = {Automated medical image segmentation is a crucial step in clinical analysis and diagnosis, as it can improve diagnostic efficiency and accuracy. Deep convolutional neural networks (DCNNs) have been widely used in the medical field, achieving excellent results. The high complexity of medical images poses a significant challenge for many networks in balancing local and global information, resulting in unstable segmentation outcomes. To address the challenge, we designed a hybrid CNN-Transformer network to capture both the local and global information. More specifically, deep convolutional neural networks are introduced to exploit the local information. At the same time, we designed a trident multi-layer fusion (TMF) block for the Transformer to fuse contextual information from higher-level (global) features dynamically. Moreover, considering the inherent characteristic of medical image segmentation (e.g., irregular shapes and discontinuous boundaries), we developed united attention (UA) blocks to focus on important feature learning. To evaluate the effectiveness of our proposed approach, we performed experiments on two publicly available datasets, ISIC-2017, and Kvasir-SEG, and compared our results with state-of-the-art approaches. The experimental results demonstrate the superior performance of our approach. The codes are available at https://github.com/Tanghui2000/HTC-Net.}
}

@ARTICLE{10.3389/frai.2021.667963,
	
	AUTHOR={Chazal, Frédéric and Michel, Bertrand},   
	
	TITLE={An Introduction to Topological Data Analysis: Fundamental and Practical Aspects for Data Scientists},      
	
	JOURNAL={Frontiers in Artificial Intelligence},      
	
	VOLUME={4},           
	
	YEAR={2021},      
	
	URL={https://www.frontiersin.org/articles/10.3389/frai.2021.667963},       
	
	DOI={10.3389/frai.2021.667963},      
	
	ISSN={2624-8212},   
	
	ABSTRACT={With the recent explosion in the amount, the variety, and the dimensionality of available data, identifying, extracting, and exploiting their underlying structure has become a problem of fundamental importance for data analysis and statistical learning. Topological data analysis (<sc>tda</sc>) is a recent and fast-growing field providing a set of new topological and geometric tools to infer relevant features for possibly complex data. It proposes new well-founded mathematical theories and computational tools that can be used independently or in combination with other data analysis and statistical learning techniques. This article is a brief introduction, through a few selected topics, to basic fundamental and practical aspects of <sc>tda</sc> for nonexperts.}
}